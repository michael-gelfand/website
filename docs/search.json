[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Group Capstone Project\nAn interactive dashboard (built with Dash) that accepts CT scan DICOM files and segments them into individual bones to support surgical planning in pediatric orthopedic cases. This capstone project was developed as part of the UBC Master of Data Science program in partnership with the Digital Lab at BC Children’s Hospital.\n\n\n\n\nGroup Academic Project\nA Plotly Dash dashboard that visualizes arrest data across New York City. Users can filter by borough or precinct and explore arrest patterns through an interactive map and dynamic charts.\nView on GitHub\nView Live Dashboard\n\n\n\n\nGroup Academic Project\nA published Python package available on PyPI that provides geographic, demographic, and economic data about countries around the world.\nView on GitHub"
  },
  {
    "objectID": "projects.html#section",
    "href": "projects.html#section",
    "title": "Projects",
    "section": "",
    "text": "Group Capstone Project\nAn interactive dashboard (built with Dash) that accepts CT scan DICOM files and segments them into individual bones to support surgical planning in pediatric orthopedic cases. This capstone project was developed as part of the UBC Master of Data Science program in partnership with the Digital Lab at BC Children’s Hospital.\n\n\n\n\nGroup Academic Project\nA Plotly Dash dashboard that visualizes arrest data across New York City. Users can filter by borough or precinct and explore arrest patterns through an interactive map and dynamic charts.\nView on GitHub\nView Live Dashboard\n\n\n\n\nGroup Academic Project\nA published Python package available on PyPI that provides geographic, demographic, and economic data about countries around the world.\nView on GitHub"
  },
  {
    "objectID": "projects.html#section-1",
    "href": "projects.html#section-1",
    "title": "Projects",
    "section": "2024",
    "text": "2024\n\nNHL Player Shooting Hand Predictor\nGroup Academic Project\nA machine learning project to predict whether an NHL player shoots left or right based on height and weight. The project includes fully reproducible analysis and containerized setup using Docker.\nView on GitHub"
  },
  {
    "objectID": "blog_posts/hp_optimization_tutorial/index.html",
    "href": "blog_posts/hp_optimization_tutorial/index.html",
    "title": "A Brief Tutorial on Hyperparameter Optimization in Python",
    "section": "",
    "text": "In today’s world of data science, machine learning is all around us and as data scientists, it is our job to get the absolute most use out of these fascinating tools. Building machine learning models with high accuracies can be a difficult task. You have already spent valuable time finding the perfect model, cleaning and preparing the data, and somehow your model still is not performing how you had hoped. What else can be done to improve our models? One important step that can sometimes be overlooked is hyperparameter optimization, and this might just be the step to take your model from good to great.\nEvery problem that machine learning hopes to solve is unique and the settings that work for one dataset might not work well for another. These hyperparameters, such as learning rates, tree depths, or regularization strengths, control how your model learns and how well it is able to generalize from your training data. Hyperparameter optimization seeks to fine tune these setting to ensure that your model performs using the best settings for the data.\nI’m Michael, a data science masters student at the University of British Columbia and I will be walking you through a tutorial on how to implement hyperparameter optimization in Python. This tutorial will show you how to implement different optimization methods such as grid search and random search and highlight the performance gains seen by using these tools. By the end of this guide, you will have the tools necessary to improve the performance of your machine learning models."
  },
  {
    "objectID": "blog_posts/hp_optimization_tutorial/index.html#what-are-hyperparameters",
    "href": "blog_posts/hp_optimization_tutorial/index.html#what-are-hyperparameters",
    "title": "A Brief Tutorial on Hyperparameter Optimization in Python",
    "section": "What are Hyperparameters",
    "text": "What are Hyperparameters\nHyperparameters are the internal configurations that control how machine learning models run. These settings can change how the model is able to generalize on unseen data through its ability to learn patterns in your training data. Each machine learning model will have a different set of hyperparameters. I don’t have enough time to go over all of them but I will cover a few that are fairly important.\nLearning rate is something that can be directly controlled by hyperparameters and decides the step size for when the model is trying to minimize the loss function. If your learning rate is too small, it will take too long to find the smallest loss however if your learning rate is too large, the model could overshoot the best solution or it might not find it at all. Another important hyperparameter is regularization strength, which helps limit overfitting by penalizing large weights. It’s key to choose the right value for regularization strength to find the right balance between model performance and complexity. Another hyperparameter is class weights, which control how much importance the model gives different classes. This is especially useful in datasets where the classes are imbalanced to prevent the dominant class from overpowering the others in decision making."
  },
  {
    "objectID": "blog_posts/hp_optimization_tutorial/index.html#finding-the-best-hyperparameters",
    "href": "blog_posts/hp_optimization_tutorial/index.html#finding-the-best-hyperparameters",
    "title": "A Brief Tutorial on Hyperparameter Optimization in Python",
    "section": "Finding the Best Hyperparameters",
    "text": "Finding the Best Hyperparameters\nFinding the best values for hyperparameters on your own would not be an easy task. It would be a long process of trial and error to find the best performing model. Thankfully, there exist automated processes for taking your model and find the best settings for it. Two methods that I will be touching on below are grid search and random search and each one has its own strengths and weaknesses.\n\nGrid Search\nGrid search is more of an exhaustive method. It will take in a grid of possible hyperparameter values and then evaluate the model on all possible combinations of these values. Say you try grid search and pass in 3 values for learning rate, 3 for regularization strength, and 2 for class weights. This method will try all 18 combinations to find the values that work best for your data. One downside is that this can get computationally expensive fast, and it can take long to find all combinations for a large pool of possible values. This method is best used when you have a rough idea of what range the values should fall into so that you are limiting your search.\n\n\nRandom Search\nRandom search takes in specified distributions for the desired hyperparameters and samples the combinations randomly to find the best result. For example, instead of evaluating all possible values for regularization strength, it would randomly pick values within a range of say 0.1 to 1.0. This method is not guaranteed to find the best solution like grid search does, however it does produce fairly desirable results and in much shorter time. Random search is best used when you aren’t sure of which hyperparameters are the most important to your model, or if you are unsure of a narrower range for these values."
  },
  {
    "objectID": "blog_posts/hp_optimization_tutorial/index.html#tutorial-performing-hyperparameter-optimization-in-python",
    "href": "blog_posts/hp_optimization_tutorial/index.html#tutorial-performing-hyperparameter-optimization-in-python",
    "title": "A Brief Tutorial on Hyperparameter Optimization in Python",
    "section": "Tutorial: Performing Hyperparameter Optimization in Python",
    "text": "Tutorial: Performing Hyperparameter Optimization in Python\n\n1. Setup and dataset\nThe dataset we will use for this tutorial is the NHL team dataset provided by TidyTuesday. Before we jump in there are a couple of things that we need to do first to ensure we can run the code. This tutorial assumes that you have conda installed on your machine and that you have some knowledge of environments.\nRequired Libraries:\n\nscikit-learn\npandas\nnumpy\n\nFirst you will need to create a new Jupyter notebook and ensure that it is running on an environment that contains the required libraries. Next you will need to run the following code to import the libraries into your notebook:\n# Load necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nNow lets import the dataset, and take a quick look at it. For this tutorial, we will be doing a classification using a decision tree model to predict what hand a hockey player shoots with based on their height and weight.\n# Read in dataset from GitHub\nrosters = pd.read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/nhl_rosters.csv\")\n\n# Data wrangling and cleanup\nrosters_clean = rosters[[\n    \"weight_in_kilograms\",\n    \"height_in_centimeters\",\n    \"shoots_catches\"\n]]\nrosters_clean = rosters_clean.dropna()\nrosters_clean[\"shoots_left\"] = (rosters_clean[\"shoots_catches\"]\n                                .replace({'L': True, 'R': False})\n                                .astype(bool)\n                               )\nrosters_clean = rosters_clean.drop(\"shoots_catches\", axis=1)\n\n# Split into train and test\ntrain_df, test_df = train_test_split(rosters_clean, test_size=0.3, random_state=123)\ntrain_df.head()\n\n\n\nTable 1: A preview of the training data showing the two explanatory variables and the target variable.\n\n\n\n\n\nweight_in_kilograms\nheight_in_centimeters\nshoots_left\n\n\n\n\n87\n183\nTrue\n\n\n92\n188\nFalse\n\n\n88\n185\nTrue\n\n\n84\n180\nFalse\n\n\n93\n188\nFalse\n\n\n\n\n\n\n\n\n2. Preprocessing\nThe next steps will be to preprocess the data to ensure that everything is scaled properly.\n# Lists of feature names\nnumeric_features = [\"weight_in_kilograms\", \"height_in_centimeters\"]\n\n# Create the column transformer\npreprocessor = make_column_transformer(\n    (StandardScaler(), numeric_features),  # Scaling on numeric features\n)\n\n# Create X and y data frames for train and test data\nX_train = train_df.drop(columns=[\"shoots_left\"])\nX_test = test_df.drop(columns=[\"shoots_left\"])\ny_train = train_df[\"shoots_left\"]\ny_test = test_df[\"shoots_left\"]\n\n\n3. Baseline Model\nFinally we can evaluate our model to see how it does:\nbaseline_model = make_pipeline(\n    preprocessor,\n    DecisionTreeClassifier(random_state=123, max_depth=2)\n)\nbaseline_model.fit(X_train, y_train)\ny_pred_baseline = baseline_model.predict(X_test)\nbaseline_accuracy = accuracy_score(y_test, y_pred_baseline)\nprint(f\"Baseline Accuracy: {baseline_accuracy:.2f}\")\nOutput:\nBaseline Accuracy: 0.65\nWe can see that we have an accuracy of 0.65. Not great but hopefully we can get some improvement with our hyperparameter optimization. These next steps will show you how to optimize with either a grid or a random search.\n\n\n4. Hyperparameter Optimization\n\n4a. Grid Search\nWe will first try optimizing using a grid search method. For that, we first need to decide which hyperparameters we want to optimize. Since we are working with a decision tree classifier, we will be focusing on 3 hyperparameters:\n\nmax_depth: The maximum depth of the decision tree\nmin_samples_split: The minimum number of samples needed to split a node further. This helps to control overfitting and prevents the tree from getting to complex.\nmin_samples_leaf: The minimum number of samples for a leaf node. This also helps to control overfitting.\n\nHere we will define our parameter grid with some possible values to try\n# Define the hyperparameter grid\nparam_grid = {\n    'decisiontreeclassifier__max_depth': [2, 4, 6, 8, 10],\n    'decisiontreeclassifier__min_samples_split': [2, 5, 10, 15, 20],\n    'decisiontreeclassifier__min_samples_leaf': [1, 2, 4, 6, 8, 10]\n}\nNext we will need to create our grid search object. The first two arguments it takes are our model and our parameter grid. The rest of the arguments are:\n\ncv: The cross-validation strategy. Here we are using 5-fold cross validation to ensure we aren’t overfitting on the data by using just one configuration of the training set.\nn_jobs: The number of jobs to run in parallel. Here we are using -1 which means we are using all our available processors to run jobs in parallel.\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(\n    make_pipeline(preprocessor, DecisionTreeClassifier(random_state=123)),\n    param_grid,\n    cv=5,\n    n_jobs=-1\n)\nNow that we have our grid search object, we can treat it just like a model and train it with our training data.\n# Fit GridSearchCV\ngrid_search.fit(X_train, y_train)\nNow that we have trained our model, we want to find out what our best hyperparameter values were and get our best performing model so that we can score it.\n# Evaluate the best model from GridSearchCV\ngrid_best_model = grid_search.best_estimator_\nprint(\"Grid search best parameters:\", grid_search.best_params_)\nOutput:\nGrid search best parameters: {\n    'decisiontreeclassifier__max_depth': 10,\n    'decisiontreeclassifier__min_samples_leaf': 1,\n    'decisiontreeclassifier__min_samples_split': 5\n}\nFrom this, we can see that our best parameters values were:\n\nmax_depth: 10\nmin_samples_split: 5\nmin_samples_leaf: 1\n\nFinally, let’s see what kind of accuracy we get with these optimized hyperparameter values.\ny_pred_grid = grid_best_model.predict(X_test)\ngrid_accuracy = accuracy_score(y_test, y_pred_grid)\nprint(f\"Grid Search Accuracy: {grid_accuracy:.2f}\")\nOutput:\nGrid Search Accuracy: 0.66\nOkay, so it looks like we only went from an accuracy of 0.65 to 0.66, which may not seem like much, but in machine learning, every improvement in performance is valuable for ensuring our model works best on unseen data.\n\n\n4b. Random Search\nWe will now try optimizing using a random search method. Once again, we will need to determine which hyperparameters we will want to optimize. For this example, we will optimize the same ones that we choose in our grid search example.\nNow we need to define a distribution of values for our random search to pull values from for our combinations. Since all our possible values are discrete, creating a range is pretty simple using numpy.\n# Define the hyperparameter distribution\nparam_dist = {\n    'decisiontreeclassifier__max_depth': np.arange(2, 20),\n    'decisiontreeclassifier__min_samples_split': np.arange(2, 20),\n    'decisiontreeclassifier__min_samples_leaf': np.arange(1, 10)\n}\nNext we will need to create our random search object. Like GridSearchCV, our first two arguments are our model and our parameter distributions. We also use cv, and n_jobs again which we saw with our grid search. Our new arguments are:\n\nn_iter: The number of iterations to run our random search. More iterations means we are more likely to find the best value at the cost of runtime. Here we are choosing 50 iterations, so we are taking 50 combinations of hyperparameters.\nrandom_state: This ensures that we get the same results each time as we are taking random combinations across our distributions.\n\n# Initialize RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    make_pipeline(preprocessor, DecisionTreeClassifier(random_state=123)),\n    param_distributions=param_dist,\n    n_iter=50,  # Number of random samples\n    cv=5,\n    n_jobs=-1,\n    random_state=123\n)\nNow that we have our random search object, the rest is pretty much the same as with our grid search. Here we will fit the random search object on the training data.\n# Fit RandomizedSearchCV\nrandom_search.fit(X_train, y_train)\nLet’s see what our best hyperparameter values were this time.\n# Evaluate the best model from RandomizedSearchCV\nprint(\"Random search best parameters:\", random_search.best_params_)\nrandom_best_model = random_search.best_estimator_\nOutput:\nRandom search best parameters: {\n    'decisiontreeclassifier__min_samples_split': 2,\n    'decisiontreeclassifier__min_samples_leaf': 2,\n    'decisiontreeclassifier__max_depth': 17\n}\nFrom this, we can see that our best parameters values were:\n\nmax_depth: 17\nmin_samples_split: 2\nmin_samples_leaf: 2\n\nNotice how these values are different from those we found using grid search. 17 wasn’t a possible option for grid search so it never explored it. This is one of the reasons why random search can be better as we didn’t know what the best range for max_depth was.\nFinally, let’s see what kind of accuracy we get with these optimized hyperparameter values.\ny_pred_random = random_best_model.predict(X_test)\nrandom_accuracy = accuracy_score(y_test, y_pred_random)\nprint(f\"Random Search Accuracy: {random_accuracy:.2f}\")\nOutput:\nRandom Search Accuracy: 0.67\nOkay, so it looks like we went from an accuracy of 0.65 to 0.67. Once again, this is not much but we can see that we achieved better results than if we were using a grid search.\n\n\n\n\n\n\nFigure 1: Comparison of model accuracies on test data for different implementations of hyperparameter optimization. Image generated by author.\n\n\n\nHyperparameter optimization is a powerful tool that can significantly improve the performance of machine learning models. In this tutorial, we went over two different methods for optimizing hyperparameters of a decision tree classifier and evaluated the results which can be seen in Figure 1. This figure shows that both of our methods made improvements to the baseline model containing no optimization. Although these increases in accuracy may appear small, as data scientists, it is important to make every improvement possible to get the most out of our models. Now that you have the tools to perform hyperparameter optimization, go out and apply them to unlock the full potential of your models!"
  },
  {
    "objectID": "blog_posts/hp_optimization_tutorial/index.html#references",
    "href": "blog_posts/hp_optimization_tutorial/index.html#references",
    "title": "A Brief Tutorial on Hyperparameter Optimization in Python",
    "section": "References",
    "text": "References\n\nUBC Master of Data Science. (n.d.). 522 Group 27: NHL player shooting hand predictor. GitHub. Retrieved January 17, 2025, from https://github.com/UBC-MDS/522_group_27\nscikit-learn developers. (n.d.). sklearn.tree.DecisionTreeClassifier. scikit-learn. Retrieved January 17, 2025, from https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html\nscikit-learn developers. (n.d.). sklearn.model_selection.GridSearchCV. scikit-learn. Retrieved January 17, 2025, from https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\nscikit-learn developers. (n.d.). sklearn.model_selection.RandomizedSearchCV. scikit-learn. Retrieved January 17, 2025, from https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\nTidyTuesday. (2024). NHL team dataset. GitHub. Retrieved January 17, 2025, from https://github.com/rfordatascience/tidytuesday"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Michael’s Blog",
    "section": "",
    "text": "A Brief Tutorial on Hyperparameter Optimization in Python\n\n\n\n\n\n\nData Science\n\n\nTutorials\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nMichael Gelfand\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michael Gelfand",
    "section": "",
    "text": "Hey, I’m Michael! I’m a data scientist with a background in electrical and computer engineering, and a passion for solving real-world problems with data.\nI hold a Bachelor of Science in Electrical Engineering (minor in Computer Engineering) from the University of Calgary, and I’m currently completing my Master of Data Science at the University of British Columbia (July 2025).\nI previously worked as a data engineer building ETL pipelines for large-scale analytics, and I’m especially interested in applying machine learning in finance, from algorithmic trading to credit risk modeling.\nWhen I’m not writing code, you’ll find me skiing, golfing, or traveling."
  }
]