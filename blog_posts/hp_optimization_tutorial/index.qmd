---
title: "A Brief Tutorial on Hyperparameter Optimization in Python"
author: "Michael Gelfand"
date: "2025-01-17"
categories: [Data Science, Tutorials, Python]
image: "header_image.jpg"
---

In today's world of data science, machine learning is all around us and as data scientists, it is our job to get the absolute most use out of these fascinating tools. Building machine learning models with high accuracies can be a difficult task. You have already spent valuable time finding the perfect model, cleaning and preparing the data, and somehow your model still is not performing how you had hoped. What else can be done to improve our models? One important step that can sometimes be overlooked is hyperparameter optimization, and this might just be the step to take your model from good to great. 

Every problem that machine learning hopes to solve is unique and the settings that work for one dataset might not work well for another. These hyperparameters, such as learning rates, tree depths, or regularization strengths, control how your model learns and how well it is able to generalize from your training data. Hyperparameter optimization seeks to fine tune these setting to ensure that your model performs using the best settings for the data. 

I'm Michael, a data science masters student at the University of British Columbia and I will be walking you through a tutorial on how to implement hyperparameter optimization in Python. This tutorial will show you how to implement different optimization methods such as grid search and random search and highlight the performance gains seen by using these tools. By the end of this guide, you will have the tools necessary to improve the performance of your machine learning models.

## What are Hyperparameters

Hyperparameters are the internal configurations that control how machine learning models run. These settings can change how the model is able to generalize on unseen data through its ability to learn patterns in your training data. Each machine learning model will have a different set of hyperparameters. I don't have enough time to go over all of them but I will cover a few that are fairly important. 

Learning rate is something that can be directly controlled by hyperparameters and decides the step size for when the model is trying to minimize the loss function. If your learning rate is too small, it will take too long to find the smallest loss however if your learning rate is too large, the model could overshoot the best solution or it might not find it at all. Another important hyperparameter is regularization strength, which helps limit overfitting by penalizing large weights. It's key to choose the right value for regularization strength to find the right balance between model performance and complexity. Another hyperparameter is class weights, which control how much importance the model gives different classes. This is especially useful in datasets where the classes are imbalanced to prevent the dominant class from overpowering the others in decision making.

## Finding the Best Hyperparameters

Finding the best values for hyperparameters on your own would not be an easy task. It would be a long process of trial and error to find the best performing model. Thankfully, there exist automated processes for taking your model and find the best settings for it. Two methods that I will be touching on below are grid search and random search and each one has its own strengths and weaknesses.

### Grid Search

Grid search is more of an exhaustive method. It will take in a grid of possible hyperparameter values and then evaluate the model on all possible combinations of these values. Say you try grid search and pass in 3 values for learning rate, 3 for regularization strength, and 2 for class weights. This method will try all 18 combinations to find the values that work best for your data. One downside is that this can get computationally expensive fast, and it can take long to find all combinations for a large pool of possible values. This method is best used when you have a rough idea of what range the values should fall into so that you are limiting your search.

### Random Search

Random search takes in specified distributions for the desired hyperparameters and samples the combinations randomly to find the best result. For example, instead of evaluating all possible values for regularization strength, it would randomly pick values within a range of say 0.1 to 1.0. This method is not guaranteed to find the best solution like grid search does, however it does produce fairly desirable results and in much shorter time. Random search is best used when you aren't sure of which hyperparameters are the most important to your model, or if you are unsure of a narrower range for these values.

## Tutorial: Performing Hyperparameter Optimization in Python

### 1. Setup and dataset

The dataset we will use for this tutorial is the NHL team dataset provided by TidyTuesday. Before we jump in there are a couple of things that we need to do first to ensure we can run the code. This tutorial assumes that you have conda installed on your machine and that you have some knowledge of environments.

Required Libraries:

- scikit-learn
- pandas
- numpy

First you will need to create a new Jupyter notebook and ensure that it is running on an environment that contains the required libraries. Next you will need to run the following code to import the libraries into your notebook:

```python
# Load necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
import numpy as np
```

Now lets import the dataset, and take a quick look at it. For this tutorial, we will be doing a classification using a decision tree model to predict what hand a hockey player shoots with based on their height and weight.

```python
# Read in dataset from GitHub
rosters = pd.read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/nhl_rosters.csv")

# Data wrangling and cleanup
rosters_clean = rosters[[
    "weight_in_kilograms",
    "height_in_centimeters",
    "shoots_catches"
]]
rosters_clean = rosters_clean.dropna()
rosters_clean["shoots_left"] = (rosters_clean["shoots_catches"]
                                .replace({'L': True, 'R': False})
                                .astype(bool)
                               )
rosters_clean = rosters_clean.drop("shoots_catches", axis=1)

# Split into train and test
train_df, test_df = train_test_split(rosters_clean, test_size=0.3, random_state=123)
train_df.head()
```

: A preview of the training data showing the two explanatory variables and the target variable. {#tbl-train}

| weight_in_kilograms | height_in_centimeters | shoots_left |
|---------------------|-----------------------|-------------|
| 87                 | 183                  | True        |
| 92                 | 188                  | False       |
| 88                 | 185                  | True        |
| 84                 | 180                  | False       |
| 93                 | 188                  | False       |


### 2. Preprocessing 

The next steps will be to preprocess the data to ensure that everything is scaled properly.

```python
# Lists of feature names
numeric_features = ["weight_in_kilograms", "height_in_centimeters"]

# Create the column transformer
preprocessor = make_column_transformer(
    (StandardScaler(), numeric_features),  # Scaling on numeric features
)

# Create X and y data frames for train and test data
X_train = train_df.drop(columns=["shoots_left"])
X_test = test_df.drop(columns=["shoots_left"])
y_train = train_df["shoots_left"]
y_test = test_df["shoots_left"]
```

### 3. Baseline Model

Finally we can evaluate our model to see how it does:

```python
baseline_model = make_pipeline(
    preprocessor,
    DecisionTreeClassifier(random_state=123, max_depth=2)
)
baseline_model.fit(X_train, y_train)
y_pred_baseline = baseline_model.predict(X_test)
baseline_accuracy = accuracy_score(y_test, y_pred_baseline)
print(f"Baseline Accuracy: {baseline_accuracy:.2f}")
```

Output:
```python
Baseline Accuracy: 0.65
```

We can see that we have an accuracy of 0.65. Not great but hopefully we can get some improvement with our hyperparameter optimization. These next steps will show you how to optimize with either a grid or a random search.

### 4. Hyperparameter Optimization

#### 4a. Grid Search

We will first try optimizing using a grid search method. For that, we first need to decide which hyperparameters we want to optimize. Since we are working with a decision tree classifier, we will be focusing on 3 hyperparameters:

- max_depth: The maximum depth of the decision tree
- min_samples_split: The minimum number of samples needed to split a node further. This helps to control overfitting and prevents the tree from getting to complex.
- min_samples_leaf: The minimum number of samples for a leaf node. This also helps to control overfitting.

Here we will define our parameter grid with some possible values to try

```python
# Define the hyperparameter grid
param_grid = {
    'decisiontreeclassifier__max_depth': [2, 4, 6, 8, 10],
    'decisiontreeclassifier__min_samples_split': [2, 5, 10, 15, 20],
    'decisiontreeclassifier__min_samples_leaf': [1, 2, 4, 6, 8, 10]
}
```

Next we will need to create our grid search object. The first two arguments it takes are our model and our parameter grid. The rest of the arguments are:

- cv: The cross-validation strategy. Here we are using 5-fold cross validation to ensure we aren't overfitting on the data by using just one configuration of the training set.
- n_jobs: The number of jobs to run in parallel. Here we are using -1 which means we are using all our available processors to run jobs in parallel.

```python
# Initialize GridSearchCV
grid_search = GridSearchCV(
    make_pipeline(preprocessor, DecisionTreeClassifier(random_state=123)),
    param_grid,
    cv=5,
    n_jobs=-1
)
```

Now that we have our grid search object, we can treat it just like a model and train it with our training data.

```python
# Fit GridSearchCV
grid_search.fit(X_train, y_train)
```

Now that we have trained our model, we want to find out what our best hyperparameter values were and get our best performing model so that we can score it.

```python
# Evaluate the best model from GridSearchCV
grid_best_model = grid_search.best_estimator_
print("Grid search best parameters:", grid_search.best_params_)
```

Output:
```python
Grid search best parameters: {
    'decisiontreeclassifier__max_depth': 10,
    'decisiontreeclassifier__min_samples_leaf': 1,
    'decisiontreeclassifier__min_samples_split': 5
}
```
From this, we can see that our best parameters values were:

- max_depth: 10
- min_samples_split: 5
- min_samples_leaf: 1

Finally, let's see what kind of accuracy we get with these optimized hyperparameter values.

```python
y_pred_grid = grid_best_model.predict(X_test)
grid_accuracy = accuracy_score(y_test, y_pred_grid)
print(f"Grid Search Accuracy: {grid_accuracy:.2f}")
```

Output:
```python
Grid Search Accuracy: 0.66
```

Okay, so it looks like we only went from an accuracy of 0.65 to 0.66, which may not seem like much, but in machine learning, every improvement in performance is valuable for ensuring our model works best on unseen data.

#### 4b. Random Search

We will now try optimizing using a random search method. Once again, we will need to determine which hyperparameters we will want to optimize. For this example, we will optimize the same ones that we choose in our grid search example.

Now we need to define a distribution of values for our random search to pull values from for our combinations. Since all our possible values are discrete, creating a range is pretty simple using numpy.

```python
# Define the hyperparameter distribution
param_dist = {
    'decisiontreeclassifier__max_depth': np.arange(2, 20),
    'decisiontreeclassifier__min_samples_split': np.arange(2, 20),
    'decisiontreeclassifier__min_samples_leaf': np.arange(1, 10)
}
```

Next we will need to create our random search object. Like GridSearchCV, our first two arguments are our model and our parameter distributions. We also use cv, and n_jobs again which we saw with our grid search. Our new arguments are:

- n_iter: The number of iterations to run our random search. More iterations means we are more likely to find the best value at the cost of runtime. Here we are choosing 50 iterations, so we are taking 50 combinations of hyperparameters.
- random_state: This ensures that we get the same results each time as we are taking random combinations across our distributions.

```python
# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    make_pipeline(preprocessor, DecisionTreeClassifier(random_state=123)),
    param_distributions=param_dist,
    n_iter=50,  # Number of random samples
    cv=5,
    n_jobs=-1,
    random_state=123
)
```

Now that we have our random search object, the rest is pretty much the same as with our grid search. Here we will fit the random search object on the training data.

```python
# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)
```

Let's see what our best hyperparameter values were this time.

```python
# Evaluate the best model from RandomizedSearchCV
print("Random search best parameters:", random_search.best_params_)
random_best_model = random_search.best_estimator_
```

Output:
```python
Random search best parameters: {
    'decisiontreeclassifier__min_samples_split': 2,
    'decisiontreeclassifier__min_samples_leaf': 2,
    'decisiontreeclassifier__max_depth': 17
}
```

From this, we can see that our best parameters values were:

- max_depth: 17
- min_samples_split: 2
- min_samples_leaf: 2

Notice how these values are different from those we found using grid search. 17 wasn't a possible option for grid search so it never explored it. This is one of the reasons why random search can be better as we didn't know what the best range for max_depth was.

Finally, let's see what kind of accuracy we get with these optimized hyperparameter values.

```python
y_pred_random = random_best_model.predict(X_test)
random_accuracy = accuracy_score(y_test, y_pred_random)
print(f"Random Search Accuracy: {random_accuracy:.2f}")
```

Output:
```python
Random Search Accuracy: 0.67
```

Okay, so it looks like we went from an accuracy of 0.65 to 0.67. Once again, this is not much but we can see that we achieved better results than if we were using a grid search.

![Comparison of model accuracies on test data for different implementations of hyperparameter optimization. Image generated by author.](compare_accuracy.png){#fig-compare_accuracy width=98%}

Hyperparameter optimization is a powerful tool that can significantly improve the performance of machine learning models. In this tutorial, we went over two different methods for optimizing hyperparameters of a decision tree classifier and evaluated the results which can be seen in @fig-compare_accuracy. This figure shows that both of our methods made improvements to the baseline model containing no optimization. Although these increases in accuracy may appear small, as data scientists, it is important to make every improvement possible to get the most out of our models. Now that you have the tools to perform hyperparameter optimization, go out and apply them to unlock the full potential of your models!

## References

1. UBC Master of Data Science. (n.d.). *522 Group 27: NHL player shooting hand predictor*. GitHub. Retrieved January 17, 2025, from [https://github.com/UBC-MDS/522_group_27](https://github.com/UBC-MDS/522_group_27)
2. scikit-learn developers. (n.d.). *sklearn.tree.DecisionTreeClassifier*. scikit-learn. Retrieved January 17, 2025, from [https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
3. scikit-learn developers. (n.d.). *sklearn.model_selection.GridSearchCV*. scikit-learn. Retrieved January 17, 2025, from [https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html)
4. scikit-learn developers. (n.d.). *sklearn.model_selection.RandomizedSearchCV*. scikit-learn. Retrieved January 17, 2025, from [https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html](https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)
5. TidyTuesday. (2024). *NHL team dataset*. GitHub. Retrieved January 17, 2025, from [https://github.com/rfordatascience/tidytuesday](https://github.com/rfordatascience/tidytuesday)